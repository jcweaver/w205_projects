{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Tracking User Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linux Commands Used\n",
    "\n",
    "#### Setting up my assignment by copying/downloading files\n",
    "```\n",
    "cp ~/w205/course-content/08-Querying-Data/docker-compose.yml .\n",
    "curl -L -o assessment-attempts-20180128-121051-nested.json https://goo.gl/ME6hjp\n",
    "\n",
    "```\n",
    "\n",
    "#### Bring up the cluster and verify it's running without any stray clusters\n",
    "```\n",
    "docker-compose up -d\n",
    "docker-compose ps\n",
    "docker ps -a\n",
    "```\n",
    "\n",
    "#### Creating a symbolic link\n",
    "```\n",
    "docker-compose exec spark bash\n",
    "ln -s /w205 w205\n",
    "exit\n",
    "```\n",
    "\n",
    "#### Running my Jupyter notebook in PySpark\n",
    "```\n",
    "docker-compose exec spark env PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 8888 --ip 0.0.0.0 --allow-root' pyspark\n",
    "```\n",
    "\n",
    "#### Checking if Hadoop is good\n",
    "```\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "```\n",
    "\n",
    "#### Creating a topic and then publishing that topic to kafka\n",
    "```\n",
    "docker-compose exec kafka kafka-topics --create --topic assessments --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
    "docker-compose exec mids bash -c \"cat /w205/project-2-jcweaver/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessments\"\n",
    "```\n",
    "\n",
    "#### Checking that the data is on the topic\n",
    "```\n",
    "docker-compose exec mids bash -c \"kafkacat -C -b kafka:29092 -t assessments -o beginning -e\"\n",
    "```\n",
    "\n",
    "#### Breaking down the cluster\n",
    "```\n",
    "docker-compose down\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Spark freebie commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.18.0.5:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9cbc031ac8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.18.0.5:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before beginning to answer business questions with this data, we need to set it up for querying. This involves:\n",
    "* Subscribing to the topic, assessments, we added to Kafka\n",
    "* Extracting the binary json data from this topic into a string in a new dataframe\n",
    "* Registering the dataframe as a temporary table to then use SQL to query against"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subscribing to the topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_assessments = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:29092\").option(\"subscribe\",\"assessments\").option(\"startingOffsets\", \"earliest\").option(\"endingOffsets\", \"latest\").load() \n",
    "\n",
    "raw_assessments.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_assessments.printSchema() #This is the kafka schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the binary json data into a string in a new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Extracting the binary json data into a string in a new dataframe\n",
    "assessments = raw_assessments.select(raw_assessments.value.cast('string'))\n",
    "assessments.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking there's stuff in the dataframe\n",
    "assessments.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unrolling the JSON into a new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- base_exam_id: string (nullable = true)\n",
      " |-- certification: string (nullable = true)\n",
      " |-- exam_name: string (nullable = true)\n",
      " |-- keen_created_at: string (nullable = true)\n",
      " |-- keen_id: string (nullable = true)\n",
      " |-- keen_timestamp: string (nullable = true)\n",
      " |-- max_attempts: string (nullable = true)\n",
      " |-- sequences: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: array (valueContainsNull = true)\n",
      " |    |    |-- element: map (containsNull = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: boolean (valueContainsNull = true)\n",
      " |-- started_at: string (nullable = true)\n",
      " |-- user_exam_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#First lambda transform to make a dataframe\n",
    "extracted_assessments = assessments.rdd.map(lambda x: Row(**json.loads(x.value))).toDF()\n",
    "extracted_assessments.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "|        base_exam_id|certification|           exam_name|   keen_created_at|             keen_id|    keen_timestamp|max_attempts|           sequences|          started_at|        user_exam_id|\n",
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717442.735266|5a6745820eb8ab000...| 1516717442.735266|         1.0|Map(questions -> ...|2018-01-23T14:23:...|6d4089e4-bde5-4a2...|\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717377.639827|5a674541ab6b0a000...| 1516717377.639827|         1.0|Map(questions -> ...|2018-01-23T14:21:...|2fec1534-b41f-441...|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...| 1516738973.653394|5a67999d3ed3e3000...| 1516738973.653394|         1.0|Map(questions -> ...|2018-01-23T20:22:...|8edbc8a8-4d26-429...|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...|1516738921.1137421|5a6799694fc7c7000...|1516738921.1137421|         1.0|Map(questions -> ...|2018-01-23T20:21:...|c0ee680e-8892-4e6...|\n",
      "|6442707e-7488-11e...|        false|Introduction to B...| 1516737000.212122|5a6791e824fccd000...| 1516737000.212122|         1.0|Map(questions -> ...|2018-01-23T19:48:...|e4525b79-7904-405...|\n",
      "|8b4488de-43a5-4ff...|        false|        Learning Git| 1516740790.309757|5a67a0b6852c2a000...| 1516740790.309757|         1.0|Map(questions -> ...|2018-01-23T20:51:...|3186dafa-7acf-47e...|\n",
      "|e1f07fac-5566-4fd...|        false|Git Fundamentals ...|1516746279.3801291|5a67b627cc80e6000...|1516746279.3801291|         1.0|Map(questions -> ...|2018-01-23T22:24:...|48d88326-36a3-4cb...|\n",
      "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743820.305464|5a67ac8cb0a5f4000...| 1516743820.305464|         1.0|Map(questions -> ...|2018-01-23T21:43:...|bb152d6b-cada-41e...|\n",
      "|1a233da8-e6e5-48a...|        false|Intermediate Pyth...|  1516743098.56811|5a67a9ba060087000...|  1516743098.56811|         1.0|Map(questions -> ...|2018-01-23T21:31:...|70073d6f-ced5-4d0...|\n",
      "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743764.813107|5a67ac54411aed000...| 1516743764.813107|         1.0|Map(questions -> ...|2018-01-23T21:42:...|9eb6d4d6-fd1f-4f3...|\n",
      "|4cdf9b5f-fdb7-4a4...|        false|A Practical Intro...|1516744091.3127241|5a67ad9b2ff312000...|1516744091.3127241|         1.0|Map(questions -> ...|2018-01-23T21:45:...|093f1337-7090-457...|\n",
      "|e1f07fac-5566-4fd...|        false|Git Fundamentals ...|1516746256.5878439|5a67b610baff90000...|1516746256.5878439|         1.0|Map(questions -> ...|2018-01-23T22:24:...|0f576abb-958a-4c0...|\n",
      "|87b4b3f9-3a86-435...|        false|Introduction to M...|  1516743832.99235|5a67ac9837b82b000...|  1516743832.99235|         1.0|Map(questions -> ...|2018-01-23T21:40:...|0c18f48c-0018-450...|\n",
      "|a7a65ec6-77dc-480...|        false|   Python Epiphanies|1516743332.7596769|5a67aaa4f21cc2000...|1516743332.7596769|         1.0|Map(questions -> ...|2018-01-23T21:34:...|b38ac9d8-eef9-495...|\n",
      "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743750.097306|5a67ac46f7bce8000...| 1516743750.097306|         1.0|Map(questions -> ...|2018-01-23T21:41:...|bbc9865f-88ef-42e...|\n",
      "|e5602ceb-6f0d-11e...|        false|Python Data Struc...|1516744410.4791961|5a67aedaf34e85000...|1516744410.4791961|         1.0|Map(questions -> ...|2018-01-23T21:51:...|8a0266df-02d7-44e...|\n",
      "|e5602ceb-6f0d-11e...|        false|Python Data Struc...|1516744446.3999851|5a67aefef5e149000...|1516744446.3999851|         1.0|Map(questions -> ...|2018-01-23T21:53:...|95d4edb1-533f-445...|\n",
      "|f432e2e3-7e3a-4a7...|        false|Working with Algo...| 1516744255.840405|5a67ae3f0c5f48000...| 1516744255.840405|         1.0|Map(questions -> ...|2018-01-23T21:50:...|f9bc1eff-7e54-42a...|\n",
      "|76a682de-6f0c-11e...|        false|Learning iPython ...| 1516744023.652257|5a67ad579d5057000...| 1516744023.652257|         1.0|Map(questions -> ...|2018-01-23T21:46:...|dc4b35a7-399a-4bd...|\n",
      "|a7a65ec6-77dc-480...|        false|   Python Epiphanies|1516743398.6451161|5a67aae6753fd6000...|1516743398.6451161|         1.0|Map(questions -> ...|2018-01-23T21:35:...|d0f8249a-597e-4e1...|\n",
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Looking at what's inside the dataframe I just made\n",
    "extracted_assessments.show() #Map shows up if nested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning this dataframe into a temporary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Making this dataframe a temptable that I can query from\n",
    "extracted_assessments.registerTempTable('assessments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             keen_id|\n",
      "+--------------------+\n",
      "|5a6745820eb8ab000...|\n",
      "|5a674541ab6b0a000...|\n",
      "|5a67999d3ed3e3000...|\n",
      "|5a6799694fc7c7000...|\n",
      "|5a6791e824fccd000...|\n",
      "|5a67a0b6852c2a000...|\n",
      "|5a67b627cc80e6000...|\n",
      "|5a67ac8cb0a5f4000...|\n",
      "|5a67a9ba060087000...|\n",
      "|5a67ac54411aed000...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select keen_id from assessments limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections, I will answer the following business questions using spark SQL against the temporary table I created above and an additional temporary table I will create below.\n",
    "\n",
    "* How many different exams were offered per course?\n",
    "* How many exams had incompletely answered questions?\n",
    "* What courses had the highest ratios of incompletes, corrects, and incorrects?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many different exams were offered per course?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at some of the courses in this dataset. We can use the temporary table that I defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|exam_name                                   |\n",
      "+--------------------------------------------+\n",
      "|Normal Forms and All That Jazz Master Class |\n",
      "|Normal Forms and All That Jazz Master Class |\n",
      "|The Principles of Microservices             |\n",
      "|The Principles of Microservices             |\n",
      "|Introduction to Big Data                    |\n",
      "|Learning Git                                |\n",
      "|Git Fundamentals for Web Developers         |\n",
      "|Introduction to Python                      |\n",
      "|Intermediate Python Programming             |\n",
      "|Introduction to Python                      |\n",
      "|A Practical Introduction to React.js        |\n",
      "|Git Fundamentals for Web Developers         |\n",
      "|Introduction to Modern Front-End Development|\n",
      "|Python Epiphanies                           |\n",
      "|Introduction to Python                      |\n",
      "|Python Data Structures                      |\n",
      "|Python Data Structures                      |\n",
      "|Working with Algorithms in Python           |\n",
      "|Learning iPython Notebook                   |\n",
      "|Python Epiphanies                           |\n",
      "+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name from assessments limit 20\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some of them are repeated so let's see some distinct course names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------+\n",
      "|exam_name                                       |\n",
      "+------------------------------------------------+\n",
      "|Learning Data Modeling                          |\n",
      "|Networking for People Who Hate Networking       |\n",
      "|Introduction to Java 8                          |\n",
      "|Learning Apache Hadoop                          |\n",
      "|Learning Spring Programming                     |\n",
      "|Learning iPython Notebook                       |\n",
      "|Introduction to Python                          |\n",
      "|Learning C# Best Practices                      |\n",
      "|Introduction to Architecting Amazon Web Services|\n",
      "|A Practical Introduction to React.js            |\n",
      "|I'm a Software Architect, Now What?             |\n",
      "|Introduction to Big Data                        |\n",
      "|View Updating                                   |\n",
      "|Mastering Python - Networking and Security      |\n",
      "|Intermediate C# Programming                     |\n",
      "|Starting a Grails 3 Project                     |\n",
      "|Introduction to Apache Spark                    |\n",
      "|JavaScript Templating                           |\n",
      "|Being a Better Introvert                        |\n",
      "|Mastering Advanced Git                          |\n",
      "+------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select DISTINCT exam_name from assessments limit 20\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many distinct course names there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT exam_name)|\n",
      "+-------------------------+\n",
      "|                      103|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select COUNT(DISTINCT exam_name) from assessments\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many distinct exam ids there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|count(DISTINCT base_exam_id)|\n",
      "+----------------------------+\n",
      "|                         107|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select COUNT(DISTINCT base_exam_id) from assessments\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a few more distinct exam ids than course names, which means some courses gave more than one exam. Let's now see which courses gave more than one exam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------------------------------+\n",
      "|count|exam_name                                           |\n",
      "+-----+----------------------------------------------------+\n",
      "|2    |Great Bash                                          |\n",
      "|2    |Being a Better Introvert                            |\n",
      "|2    |Introduction to Python                              |\n",
      "|2    |Architectural Considerations for Hadoop Applications|\n",
      "|1    |Introduction to Java 8                              |\n",
      "|1    |Learning Spring Programming                         |\n",
      "|1    |Learning Apache Hadoop                              |\n",
      "|1    |Introduction to Architecting Amazon Web Services    |\n",
      "|1    |Learning C# Best Practices                          |\n",
      "|1    |Learning iPython Notebook                           |\n",
      "|1    |Networking for People Who Hate Networking           |\n",
      "|1    |JavaScript Templating                               |\n",
      "|1    |A Practical Introduction to React.js                |\n",
      "|1    |Learning DNS                                        |\n",
      "|1    |Introduction to Big Data                            |\n",
      "|1    |View Updating                                       |\n",
      "|1    |Mastering Python - Networking and Security          |\n",
      "|1    |I'm a Software Architect, Now What?                 |\n",
      "|1    |Starting a Grails 3 Project                         |\n",
      "|1    |Introduction to Apache Spark                        |\n",
      "+-----+----------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select COUNT( DISTINCT base_exam_id) as count, exam_name from assessments GROUP BY exam_name ORDER BY count DESC\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 4 courses: Introduction to Python, Great Bash, Being a Better Introvert, and Architectural Considerations for Hadoop Applications had 2 different exams. Next, I'll write this to HDFS in case we want to use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam_counts_per_course = spark.sql(\"select COUNT( DISTINCT base_exam_id) as count, exam_name from assessments GROUP BY exam_name ORDER BY count DESC\")\n",
    "exam_counts_per_course.write.parquet(\"/tmp/exam_counts_per_course\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many exams had incompletely answered questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I'll define a lambda function that will help us unroll more of the JSON that contains the counts of question responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_lambda_results_total(x):\n",
    "    \n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_list = []\n",
    "    \n",
    "    if \"sequences\" in raw_dict:\n",
    "        \n",
    "        if \"counts\" in raw_dict[\"sequences\"]:\n",
    "            \n",
    "            if \"correct\" in raw_dict[\"sequences\"][\"counts\"] and \"total\" in raw_dict[\"sequences\"][\"counts\"] and \"incomplete\" in raw_dict[\"sequences\"][\"counts\"] and \"incorrect\" in raw_dict[\"sequences\"][\"counts\"] and \"unanswered\" in raw_dict[\"sequences\"][\"counts\"] :\n",
    "                    \n",
    "                my_dict = {\"correct\": raw_dict[\"sequences\"][\"counts\"][\"correct\"], \n",
    "                           \"total\": raw_dict[\"sequences\"][\"counts\"][\"total\"],\n",
    "                          \"incomplete\" : raw_dict[\"sequences\"][\"counts\"][\"incomplete\"],\n",
    "                          \"incorrect\" : raw_dict[\"sequences\"][\"counts\"][\"incorrect\"],\n",
    "                          \"unanswered\" : raw_dict[\"sequences\"][\"counts\"][\"unanswered\"],\n",
    "                          \"exam_name\" : raw_dict[\"exam_name\"]}\n",
    "                my_list.append(Row(**my_dict))\n",
    "    \n",
    "    return my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Use the lambda function to create a dataframe\n",
    "results_total = assessments.rdd.flatMap(my_lambda_results_total).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Register the dataframe as a temptable to use for SQL\n",
    "results_total.registerTempTable('results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the different counts we have for exam results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+---------+-----+----------+\n",
      "|correct|           exam_name|incomplete|incorrect|total|unanswered|\n",
      "+-------+--------------------+----------+---------+-----+----------+\n",
      "|      2|Normal Forms and ...|         1|        1|    4|         0|\n",
      "|      1|Normal Forms and ...|         2|        1|    4|         0|\n",
      "|      3|The Principles of...|         0|        1|    4|         0|\n",
      "|      2|The Principles of...|         2|        0|    4|         0|\n",
      "|      3|Introduction to B...|         0|        1|    4|         0|\n",
      "|      5|        Learning Git|         0|        0|    5|         0|\n",
      "|      1|Git Fundamentals ...|         0|        0|    1|         0|\n",
      "|      5|Introduction to P...|         0|        0|    5|         0|\n",
      "|      4|Intermediate Pyth...|         0|        0|    4|         0|\n",
      "|      0|Introduction to P...|         1|        0|    5|         4|\n",
      "+-------+--------------------+----------+---------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from results limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the ratio of incompletely answered questions over the total questions. We are most interested in the exams that had a high ratio, which means that more of the questions were incomplete. This could be an indication that the questions are poorly worded and hard for students to comprehend how to fully answer. It could also mean the questions are difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+\n",
      "|count(total)|   incomplete_ratio|\n",
      "+------------+-------------------+\n",
      "|           4|                1.0|\n",
      "|           1|                0.8|\n",
      "|           8|               0.75|\n",
      "|           1| 0.7142857142857143|\n",
      "|          21| 0.6666666666666666|\n",
      "|           8|                0.6|\n",
      "|           4| 0.5714285714285714|\n",
      "|          40|                0.5|\n",
      "|          29|0.42857142857142855|\n",
      "|          44|                0.4|\n",
      "|          78| 0.3333333333333333|\n",
      "|           1|                0.3|\n",
      "|          38| 0.2857142857142857|\n",
      "|         457|               0.25|\n",
      "|         137|                0.2|\n",
      "|          24|0.16666666666666666|\n",
      "|          43|0.14285714285714285|\n",
      "|          15|              0.125|\n",
      "|           4|                0.1|\n",
      "|        2318|                0.0|\n",
      "+------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select COUNT(total), incomplete/total as incomplete_ratio from results GROUP BY incomplete_ratio ORDER BY incomplete_ratio DESC\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 4 instances of exams where all the questions were marked as incomplete. Most of these different ratios have pretty lower numbers of counts but there are 457 exams that had 25% of the questions answered incompletely and 137 exams that had 20% of the questions answered incompletely. These might be worth looking into further.\n",
    "\n",
    "To further investigate, let's also print the total number of questions. If the total is larger, having a smaller ratio of incompletes still means there are a lot of incompletely answered questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-----+\n",
      "|count(total)|   incomplete_ratio|total|\n",
      "+------------+-------------------+-----+\n",
      "|           1|                1.0|    4|\n",
      "|           3|                1.0|    1|\n",
      "|           1|                0.8|    5|\n",
      "|           8|               0.75|    4|\n",
      "|           1| 0.7142857142857143|    7|\n",
      "|           1| 0.6666666666666666|    6|\n",
      "|          20| 0.6666666666666666|    3|\n",
      "|           8|                0.6|    5|\n",
      "|           4| 0.5714285714285714|    7|\n",
      "|           4|                0.5|    2|\n",
      "|           1|                0.5|    8|\n",
      "|          35|                0.5|    4|\n",
      "|          29|0.42857142857142855|    7|\n",
      "|          44|                0.4|    5|\n",
      "|          13| 0.3333333333333333|    6|\n",
      "|          65| 0.3333333333333333|    3|\n",
      "|           1|                0.3|   10|\n",
      "|          38| 0.2857142857142857|    7|\n",
      "|         452|               0.25|    4|\n",
      "|           5|               0.25|    8|\n",
      "+------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select COUNT(total), incomplete/total as incomplete_ratio, total from results GROUP BY incomplete_ratio, total ORDER BY incomplete_ratio DESC\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the exams with all of the questions marked incompletely had only 1 question, so it's not as big of a deal that all of the questions were answered incomplete. It is worth looking into the exam that had 4 incompletely answered questions out of 4. Next I'll write this to HDFS in case we want to use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete_ratios = spark.sql(\"select COUNT(total) as count, incomplete/total as incomplete_ratio, total from results GROUP BY incomplete_ratio, total ORDER BY incomplete_ratio DESC\")\n",
    "incomplete_ratios.write.parquet(\"/tmp/incomplete_ratios\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What courses had the highest ratios of incompletes, corrects, and incorrects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+-----------------------------------------------------------------------+\n",
      "|count(total)|incomplete_ratio  |exam_name                                                              |\n",
      "+------------+------------------+-----------------------------------------------------------------------+\n",
      "|2           |1.0               |View Updating                                                          |\n",
      "|1           |1.0               |I'm a Software Architect, Now What?                                    |\n",
      "|1           |1.0               |Introduction to Hadoop YARN                                            |\n",
      "|1           |0.8               |Design Patterns in Java                                                |\n",
      "|1           |0.75              |Building Web Services with Java                                        |\n",
      "|3           |0.75              |Cloud Computing With AWS                                               |\n",
      "|4           |0.75              |I'm a Software Architect, Now What?                                    |\n",
      "|1           |0.7142857142857143|Introduction to Data Science with R                                    |\n",
      "|4           |0.6666666666666666|Introduction to Architecting Amazon Web Services                       |\n",
      "|1           |0.6666666666666666|Introduction to Amazon Web Services (AWS) - EC2 Deployment Fundamentals|\n",
      "|2           |0.6666666666666666|Amazon Web Services - Simple Storage Service                           |\n",
      "|3           |0.6666666666666666|Web & Native Working Together                                          |\n",
      "|1           |0.6666666666666666|Arduino Prototyping Techniques                                         |\n",
      "|1           |0.6666666666666666|Relational Theory for Computer Professionals                           |\n",
      "|9           |0.6666666666666666|Learning Java EE 7                                                     |\n",
      "|2           |0.6               |Design Patterns in Java                                                |\n",
      "|4           |0.6               |Introduction to Python                                                 |\n",
      "|1           |0.6               |Event-Driven Microservices                                             |\n",
      "|1           |0.6               |Architectural Considerations for Hadoop Applications                   |\n",
      "|4           |0.5714285714285714|Introduction to Data Science with R                                    |\n",
      "+------------+------------------+-----------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select COUNT(total), incomplete/total as incomplete_ratio, exam_name from results GROUP BY incomplete_ratio, exam_name ORDER BY incomplete_ratio DESC\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the highest incomplete ratios came from the courses: View Updating, I'm a Software Architect, Now What?, and Introduction to Hadoop YARN. These might be courses to avoid due to the difficulty answering questions on these courses' exams.\n",
    "Next, I'll write this to HDFS in case we want to use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete_ratios_by_exam = spark.sql(\"select COUNT(total) as count, incomplete/total as incomplete_ratio, exam_name from results GROUP BY incomplete_ratio, exam_name ORDER BY incomplete_ratio DESC\")\n",
    "incomplete_ratios_by_exam.write.parquet(\"/tmp/incomplete_ratios_by_exam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for the courses with the highest correct ratios. This is probably an indicator of easier courses to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+-----------------------------------------------------------+\n",
      "|count|correct_ratio|exam_name                                                  |\n",
      "+-----+-------------+-----------------------------------------------------------+\n",
      "|130  |1.0          |Learning Git                                               |\n",
      "|94   |1.0          |Introduction to Java 8                                     |\n",
      "|46   |1.0          |Introduction to Machine Learning                           |\n",
      "|29   |1.0          |Beginning Programming with JavaScript                      |\n",
      "|24   |1.0          |Advanced Machine Learning                                  |\n",
      "|23   |1.0          |Intermediate Python Programming                            |\n",
      "|21   |1.0          |Learning Apache Maven                                      |\n",
      "|21   |1.0          |Git Fundamentals for Web Developers                        |\n",
      "|21   |1.0          |Learning Eclipse                                           |\n",
      "|20   |1.0          |Learning to Program with R                                 |\n",
      "|19   |1.0          |Introduction to Python                                     |\n",
      "|15   |1.0          |Introduction to Big Data                                   |\n",
      "|14   |1.0          |Learning SQL                                               |\n",
      "|14   |1.0          |Beginning C# Programming                                   |\n",
      "|13   |1.0          |Software Architecture Fundamentals Beyond The Basics       |\n",
      "|12   |1.0          |Python Epiphanies                                          |\n",
      "|12   |1.0          |Software Architecture Fundamentals Understanding the Basics|\n",
      "|11   |1.0          |Learning DNS                                               |\n",
      "|10   |1.0          |Expert Data Wrangling with R                               |\n",
      "|10   |1.0          |Intermediate C# Programming                                |\n",
      "+-----+-------------+-----------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select COUNT(total) AS count, correct/total as correct_ratio, exam_name from results GROUP BY correct_ratio, exam_name ORDER BY correct_ratio DESC, count DESC\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Git had 130 people get all the questions correct. Introduction to Java 8 had 94 people get all questions correct. These seem to be the classes to take.\n",
    "\n",
    "Next, I'll write this to HDFS to look at later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_ratios_by_exam = spark.sql(\"select COUNT(total) AS count, correct/total as correct_ratio, exam_name from results GROUP BY correct_ratio, exam_name ORDER BY correct_ratio DESC, count DESC\")\n",
    "correct_ratios_by_exam.write.parquet(\"/tmp/correct_ratios_by_exam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's look at courses with high ratios of incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+-----------------------------------------------------------+\n",
      "|count|incorrect_ratio|exam_name                                                  |\n",
      "+-----+---------------+-----------------------------------------------------------+\n",
      "|7    |1.0            |Git Fundamentals for Web Developers                        |\n",
      "|7    |1.0            |Learning Git                                               |\n",
      "|5    |1.0            |An Introduction to d3.js: From Scattered to Scatterplot    |\n",
      "|5    |1.0            |Software Architecture Fundamentals Beyond The Basics       |\n",
      "|5    |1.0            |Nullology                                                  |\n",
      "|4    |1.0            |Intermediate Python Programming                            |\n",
      "|3    |1.0            |Introduction to Apache Hive                                |\n",
      "|3    |1.0            |Being a Better Introvert                                   |\n",
      "|3    |1.0            |Intermediate C# Programming                                |\n",
      "|3    |1.0            |Data Visualization in R with ggplot2                       |\n",
      "|3    |1.0            |Python Data Structures                                     |\n",
      "|2    |1.0            |Expert Data Wrangling with R                               |\n",
      "|2    |1.0            |Beginning C# Programming                                   |\n",
      "|2    |1.0            |Software Architecture Fundamentals Understanding the Basics|\n",
      "|2    |1.0            |Introduction to Big Data                                   |\n",
      "|2    |1.0            |Mastering Advanced Git                                     |\n",
      "|1    |1.0            |Learning Apache Cassandra                                  |\n",
      "|1    |1.0            |Learning Linux System Administration                       |\n",
      "|1    |1.0            |Learning Data Structures and Algorithms                    |\n",
      "|1    |1.0            |TCP/IP                                                     |\n",
      "+-----+---------------+-----------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select COUNT(total) AS count, incorrect/total as incorrect_ratio, exam_name from results GROUP BY incorrect_ratio, exam_name ORDER BY incorrect_ratio DESC, count DESC\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Git Fundamentals for Web Developers and Learning Git each had 7 students get all the questions incorrect on the exams. Even though we saw above that 130 people got all the questions right on Learning Git, 7 people got them all wrong. This could be concerning and worth investigating further.\n",
    "Next I'll write this to HDFS in case we want to use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "incorrect_ratios = spark.sql(\"select COUNT(total) AS count, incorrect/total as incorrect_ratio, exam_name from results GROUP BY incorrect_ratio, exam_name ORDER BY incorrect_ratio DESC, count DESC\")\n",
    "incorrect_ratios.write.parquet(\"/tmp/incorrect_ratios\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
